{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9tFN+B5xxgKFR1ZkZ1gXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khai2708/Ai_application/blob/main/Week13_Lab_1_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ns4F1JlEUAi"
      },
      "outputs": [],
      "source": [
        "# Using the TextVectorization layer\n",
        "import string\n",
        "class Vectorizer:\n",
        "  def standardize(self, text):\n",
        "    text = text.lower ()\n",
        "    return \"\". join(char for char in text if char not in string.punctuation)\n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split ()\n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in dataset:\n",
        "      text = self.standardize (text)\n",
        "      tokens = self.tokenize (text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len (self.vocabulary)\n",
        "    self.inverse_vocabulary = dict((v, k) for k, v in self. vocabulary. items ())\n",
        "  def encode (self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get (token, 1) for token in tokens]\n",
        "  def decode (self, int_sequence):\n",
        "    return \" \".join(\n",
        "        self. inverse_vocabulary.get (i, \"[UNK]\") for i in int_sequence)\n",
        "vectorizer = Vectorizer ()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Eraase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode (test_sentence)\n",
        "print(encoded_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KhxVYK0Eoni",
        "outputId": "3aecbbf8-3d1e-42b4-8cb9-4d9e3b8e58c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 8, 1, 5, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = vectorizer.decode (encoded_sentence)\n",
        "print (decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq1pmyaUFbg8",
        "outputId": "789cb7e0-a049-4777-9737-42ca7c1e7474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7MUH7gY-Kmp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "output_mode=\"int\",\n",
        ")"
      ],
      "metadata": {
        "id": "2d4WAW-XGJU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "import string\n",
        "import tensorflow as tf\n",
        "def custom_standardization_fn(string_tensor):\n",
        "  lowercase_string = tf.strings.lower (string_tensor)\n",
        "  return tf.strings.regex_replace(\n",
        "      lowercase_string,f\"[{re.escape(string.punctuation)}]\",\"\")\n",
        "def custom_split_fn(string_tensor):\n",
        "  return tf.strings.split (string_tensor)\n",
        "  text_vectorization = TextVectorization (\n",
        "      output_mode=\"int\",\n",
        "      standardize=custom_standardization_fn,\n",
        "      split=custom_split_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "CoxsHkmLGPC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "hnzfy4rqHVZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the vocabulary\n",
        "text_vectorization.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wk_GwfdHnD6",
        "outputId": "86119ee1-0e35-45c6-c29e-8175df52ce1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary=text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\" \n",
        "encoded_sentence = text_vectorization(test_sentence) \n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFn2xnNP4aMZ",
        "outputId": "cfe5e391-9e0c-4f0f-aad2-d4bb1952bafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab=dict(enumerate (vocabulary))\n",
        "decoded_sentence = \" \" .join(inverse_vocab[int(i)] for i in encoded_sentence) \n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTK17rQv4a9c",
        "outputId": "50b3c348-d119-43d4-bf92-104fca50f3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Two approaches for representing groups of words: Sets and sequences\n",
        "#Preparing the IMDB movie reviews data\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
        "!tar -xf aclImdb_v1.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6bF210t423g",
        "outputId": "c7b3cde1-ecf5-48de-8e23-bc3e4cd20748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  9227k      0  0:00:08  0:00:08 --:--:-- 17.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "dSh3yUrE5Ln9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdWtxA8e6XR2",
        "outputId": "8855b127-cc7b-4b89-eebb-cd8c14ee4975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir= base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category) \n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files [-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname,\n",
        "                val_dir/category/ fname)"
      ],
      "metadata": {
        "id": "XmrldWQp6X1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "train_ds =keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory( \n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory( \n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDxCX6Pk8IFe",
        "outputId": "032848a8-3996-49a9-b1d4-6f0fd6fb97d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the shapes and dtypes of the first batch\n",
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape) \n",
        "  print(\"inputs.dtype:\", inputs.dtype) \n",
        "  print(\"targets. shape:\", targets.shape) \n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0]) \n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC_y1_vDHE3P",
        "outputId": "df626790-eaaa-4574-c052-4e66937d78d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets. shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"WOW. If you think that a film can't fatigue in some way, then you haven't seen Dog Bite Dog. This film pulls no punches, and it doesn't shy away from showing very disturbing images at all. Much like Salo, this one shows us the dehumanization of the human spirit. It is gritty, dark, depressing and hopeless, but it is also one of the best films to ever come out of Hong Kong.<br /><br />The script is much more of the same, but don't go on thinking it is incredibly clich\\xc3\\xa9d. It basically is about a troubling and obsessive detective in a cat and mouse game, against a professional and emotionless hit-man. While the script offers nothing new on the surface, it does provide a lot of questions about the dark side of humanity. Is violence really that necessary? Do we become more or less human when we abuse a 5 year old child, without pity, without remorse? In turn, we humans act no less than rabid dogs when we are blinded by anger, this is a sad truth. It is a topic that the director brilliantly explores, without limiting himself at all. Besides the cat and mouse chase, the script also develops two separate story lines for the main characters. One is about love, and the other is about redemption. Even if the script isn't that new, it is still wonderfully written and it keeps you glued to the seat at all times. <br /><br />The acting is really, really good. Edison Chen as the Hit-man is incredible; he proves that he isn't just any pretty face. He is ruthless, vile and beyond likable. Sam Lee as the obsessed cop is also outstanding. The supporting cast, in short, is excellent. The music is also worth mentioning. Very somber score by Ben Cheung, with some effective light hearted songs played at key dark moments in the film. The cinematography by Yuen Man is also really good.<br /><br />Overall, this CATIII film is highly recommended. Very well paced, incredibly acted, marvelously scored and just really good at the end of it all. However, as many have pointed out, this is not a movie for everyone. If you dislike strong violence then you should stay away from this one. If you don't like seeing heavy negativity in film then this isn't for you too. In the end, a powerhouse film, 8/10.\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing words as a set: The bag-of-words approach\n",
        "#Single words (unigrams) with binary encoding\n",
        "#Preprocessing our datasets with a Text Vectorization layer\n",
        "text_vectorization = TextVectorization (\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x) \n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls =4)\n"
      ],
      "metadata": {
        "id": "gwLGLVwDHqEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inspecting the output of our binary unigram dataset\n",
        "for inputs, targets in binary_1gram_train_ds: \n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0]) \n",
        "  print(\"targets [0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da0--DlZIDSm",
        "outputId": "38f641d0-d1be-4207-f135-513421c3bf55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets [0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Our model-building utility\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16): \n",
        "  inputs = keras. Input (shape=(max_tokens,))\n",
        "  x = layers. Dense (hidden_dim, activation=\"relu\") (inputs)\n",
        "  x = layers. Dropout (0.5)(x)\n",
        "  outputs = layers.Dense (1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\",\n",
        "                loss=\"binary_crossentropy\", \n",
        "                metrics=[\"accuracy\"])\n",
        "  return model\n",
        "  "
      ],
      "metadata": {
        "id": "zNQ5JRgWJ6c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and testing the binary unigram model\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks. ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                     save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5ehrbE3K4X5",
        "outputId": "dab06586-70ec-420e-d1dd-7c7b32410ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 10s 12ms/step - loss: 0.4165 - accuracy: 0.8246 - val_loss: 0.2781 - val_accuracy: 0.8918\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2774 - accuracy: 0.8967 - val_loss: 0.2721 - val_accuracy: 0.8960\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2471 - accuracy: 0.9100 - val_loss: 0.2777 - val_accuracy: 0.8962\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2305 - accuracy: 0.9218 - val_loss: 0.2901 - val_accuracy: 0.8978\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2227 - accuracy: 0.9263 - val_loss: 0.3014 - val_accuracy: 0.8996\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2132 - accuracy: 0.9306 - val_loss: 0.3159 - val_accuracy: 0.8992\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2164 - accuracy: 0.9315 - val_loss: 0.3252 - val_accuracy: 0.8946\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2152 - accuracy: 0.9349 - val_loss: 0.3347 - val_accuracy: 0.8944\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2049 - accuracy: 0.9370 - val_loss: 0.3393 - val_accuracy: 0.8944\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2018 - accuracy: 0.9381 - val_loss: 0.3495 - val_accuracy: 0.8936\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.2911 - accuracy: 0.8856\n",
            "Test acc: 0.886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bigrams with binary encoding\n",
        "#Configuring the Text Vectorization layer to return bigrams\n",
        "\n",
        "text_vectorization = TextVectorization (\n",
        "    ngrams=2,\n",
        "    max_tokens=20000, \n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ],
      "metadata": {
        "id": "zL-JpKYNLfHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "END OF 1-6"
      ],
      "metadata": {
        "id": "YEiZTTvHMJal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and testing the binary bigram model\n",
        "text_vectorization.adapt (text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4) \n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4) \n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4)\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint (\"binary_2gram.keras\",\n",
        "                                     save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs = 10,\n",
        "          callbacks = callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds) [1]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ4bk3_MLx58",
        "outputId": "176835a3-ed30-4069-f949-7478f7dd2ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.3840 - accuracy: 0.8414 - val_loss: 0.2654 - val_accuracy: 0.9020\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2356 - accuracy: 0.9182 - val_loss: 0.2680 - val_accuracy: 0.9036\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1975 - accuracy: 0.9336 - val_loss: 0.2850 - val_accuracy: 0.9048\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1825 - accuracy: 0.9405 - val_loss: 0.3024 - val_accuracy: 0.9054\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1837 - accuracy: 0.9456 - val_loss: 0.3203 - val_accuracy: 0.9030\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1783 - accuracy: 0.9500 - val_loss: 0.3306 - val_accuracy: 0.9042\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1741 - accuracy: 0.9517 - val_loss: 0.3454 - val_accuracy: 0.9018\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1718 - accuracy: 0.9524 - val_loss: 0.3454 - val_accuracy: 0.9010\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1775 - accuracy: 0.9540 - val_loss: 0.3525 - val_accuracy: 0.8976\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1599 - accuracy: 0.9550 - val_loss: 0.3612 - val_accuracy: 0.8958\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.2690 - accuracy: 0.9005\n",
            "Test acc: 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bigrams with TF-IDF encoding\n",
        "#Configuring the Text Vectorization layer to return token counts\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ],
      "metadata": {
        "id": "TJ2wMhVuNH1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring Text Vectorization to return TF-IDF-weighted outputs\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ],
      "metadata": {
        "id": "cqii3LlIN5sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "end of 1-7"
      ],
      "metadata": {
        "id": "01sVolXOOMwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and testing the TF-IDF bigram model\n",
        "text_vectorization.adapt (text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4)\n",
        "model = get_model()\n",
        "model.summary ()\n",
        "callbacks =[\n",
        "    keras.callbacks.ModelCheckpoint (\"tfidf_2gram.keras\",\n",
        "                                     save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(), \n",
        "          epochs=10, \n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print (f\"Test acc: {model.evaluate(tfidf_2gram_test_ds) [1]: 3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMpIWln2OJgS",
        "outputId": "e65ef525-a164-4f8a-8dce-ea07cbcf6aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 10s 15ms/step - loss: 0.5028 - accuracy: 0.7904 - val_loss: 0.3307 - val_accuracy: 0.8862\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3346 - accuracy: 0.8645 - val_loss: 0.3153 - val_accuracy: 0.8834\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2953 - accuracy: 0.8794 - val_loss: 0.3228 - val_accuracy: 0.8768\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2735 - accuracy: 0.8848 - val_loss: 0.3362 - val_accuracy: 0.8790\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2591 - accuracy: 0.8881 - val_loss: 0.3439 - val_accuracy: 0.8750\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2496 - accuracy: 0.8899 - val_loss: 0.3529 - val_accuracy: 0.8726\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2381 - accuracy: 0.9000 - val_loss: 0.3535 - val_accuracy: 0.8886\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2364 - accuracy: 0.9031 - val_loss: 0.3540 - val_accuracy: 0.8820\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2278 - accuracy: 0.9054 - val_loss: 0.3431 - val_accuracy: 0.8902\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2238 - accuracy: 0.9077 - val_loss: 0.3529 - val_accuracy: 0.8776\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.3178 - accuracy: 0.8826\n",
            "Test acc:  0.882600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We displayed the vocabulary with text_vectorization with get_vocabulary(). And then we downloaded necessary data from stanford.edu . Then Displaying the shapes and dtypes of the first batch, \n",
        "Processing words as a set: The bag-of-words approach, single words (unigrams) with binary encoding,\n",
        "preprocessing our datasets with a Text Vectorization layer, \n",
        "training and testing the binary unigram model, \n",
        "training and testing the binary bigram model,\n",
        "Training and testing the TF-IDF bigram model.\n",
        "\n"
      ],
      "metadata": {
        "id": "MIBFVboNjoUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization( inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)\n"
      ],
      "metadata": {
        "id": "F0p69gs5PjFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print (f\"{float (predictions [0] * 100): 2f} percent positive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySkrsiFbP4XL",
        "outputId": "58dedad3-07e0-4695-ddb5-861b05ea9fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 98.680031 percent positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UhyFenrxQKiF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}